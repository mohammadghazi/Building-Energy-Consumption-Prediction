{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for data analysis and manipulation.\n",
    "import pandas as pd\n",
    "\n",
    "# to have access to mathematical and statistical functions.\n",
    "import numpy as np\n",
    "\n",
    "# For data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Missing data visualization module for Python.\n",
    "import missingno as msno\n",
    "\n",
    "#Statistical data visualization\n",
    "import seaborn as sns\n",
    "\n",
    "#to see correlation between features and how they affect each other\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "#Split arrays or matrices into random train and test subsets.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#A random forest regressor. ==> https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "#SVR algorithm regressor. ==> https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "#Coefficient of determination\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "#MSE\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "#Exhaustive search over specified parameter values for an estimator. ==> https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EnergyCons</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Time</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-01-01 01:00:00</th>\n",
       "      <td>23.783228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-01 02:00:00</th>\n",
       "      <td>23.783228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-01 03:00:00</th>\n",
       "      <td>23.783228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-01 04:00:00</th>\n",
       "      <td>23.783228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-01 05:00:00</th>\n",
       "      <td>23.783228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-31 19:00:00</th>\n",
       "      <td>18.602723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-31 20:00:00</th>\n",
       "      <td>18.838200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-31 21:00:00</th>\n",
       "      <td>18.602723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-31 22:00:00</th>\n",
       "      <td>18.131768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-31 23:00:00</th>\n",
       "      <td>18.602723</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26303 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     EnergyCons\n",
       "Time                           \n",
       "2016-01-01 01:00:00   23.783228\n",
       "2016-01-01 02:00:00   23.783228\n",
       "2016-01-01 03:00:00   23.783228\n",
       "2016-01-01 04:00:00   23.783228\n",
       "2016-01-01 05:00:00   23.783228\n",
       "...                         ...\n",
       "2018-12-31 19:00:00   18.602723\n",
       "2018-12-31 20:00:00   18.838200\n",
       "2018-12-31 21:00:00   18.602723\n",
       "2018-12-31 22:00:00   18.131768\n",
       "2018-12-31 23:00:00   18.602723\n",
       "\n",
       "[26303 rows x 1 columns]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the dataset is in an excel file which we have to load it\n",
    "Energy = pd.read_excel('/mnt/e/Education/M.Sc at UT/Projects/Energy consumption prediction/Iterative model/forGitHub/Datasets/BuildingEnergyConsumption2016_2018.xlsx')\n",
    "Energy = Energy.set_index('Time')  # for further processing, we need to set index of each smaple with Time\n",
    "Energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>month</th>\n",
       "      <th>HH</th>\n",
       "      <th>TD</th>\n",
       "      <th>U</th>\n",
       "      <th>Temp</th>\n",
       "      <th>RH</th>\n",
       "      <th>Q</th>\n",
       "      <th>DR</th>\n",
       "      <th>FF</th>\n",
       "      <th>FX</th>\n",
       "      <th>P</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-01-01 01:00:00</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>82</td>\n",
       "      <td>6.6</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>70</td>\n",
       "      <td>10224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-01 02:00:00</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>43</td>\n",
       "      <td>83</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>80</td>\n",
       "      <td>10228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-01 03:00:00</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>46</td>\n",
       "      <td>91</td>\n",
       "      <td>5.9</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>80</td>\n",
       "      <td>10232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-01 04:00:00</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>36</td>\n",
       "      <td>96</td>\n",
       "      <td>4.2</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>40</td>\n",
       "      <td>10237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-01 05:00:00</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>37</td>\n",
       "      <td>98</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>30</td>\n",
       "      <td>10240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-31 19:00:00</th>\n",
       "      <td>12</td>\n",
       "      <td>19</td>\n",
       "      <td>78</td>\n",
       "      <td>93</td>\n",
       "      <td>8.7</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>60</td>\n",
       "      <td>10341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-31 20:00:00</th>\n",
       "      <td>12</td>\n",
       "      <td>20</td>\n",
       "      <td>74</td>\n",
       "      <td>92</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>50</td>\n",
       "      <td>10338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-31 21:00:00</th>\n",
       "      <td>12</td>\n",
       "      <td>21</td>\n",
       "      <td>66</td>\n",
       "      <td>89</td>\n",
       "      <td>8.2</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>60</td>\n",
       "      <td>10336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-31 22:00:00</th>\n",
       "      <td>12</td>\n",
       "      <td>22</td>\n",
       "      <td>68</td>\n",
       "      <td>94</td>\n",
       "      <td>7.6</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>70</td>\n",
       "      <td>10332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-31 23:00:00</th>\n",
       "      <td>12</td>\n",
       "      <td>23</td>\n",
       "      <td>67</td>\n",
       "      <td>94</td>\n",
       "      <td>7.6</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>40</td>\n",
       "      <td>60</td>\n",
       "      <td>10333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26303 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     month  HH  TD   U  Temp    RH  Q  DR  FF  FX      P\n",
       "Time                                                                    \n",
       "2016-01-01 01:00:00      1   1  38  82   6.6  0.82  0   0  30  70  10224\n",
       "2016-01-01 02:00:00      1   2  43  83   7.0  0.83  0   0  40  80  10228\n",
       "2016-01-01 03:00:00      1   3  46  91   5.9  0.91  0   0  30  80  10232\n",
       "2016-01-01 04:00:00      1   4  36  96   4.2  0.96  0   0  20  40  10237\n",
       "2016-01-01 05:00:00      1   5  37  98   4.0  0.98  0   0  20  30  10240\n",
       "...                    ...  ..  ..  ..   ...   ... ..  ..  ..  ..    ...\n",
       "2018-12-31 19:00:00     12  19  78  93   8.7  0.93  0   0  30  60  10341\n",
       "2018-12-31 20:00:00     12  20  74  92   8.5  0.92  0   0  30  50  10338\n",
       "2018-12-31 21:00:00     12  21  66  89   8.2  0.89  0   0  40  60  10336\n",
       "2018-12-31 22:00:00     12  22  68  94   7.6  0.94  0   0  40  70  10332\n",
       "2018-12-31 23:00:00     12  23  67  94   7.6  0.94  0   7  40  60  10333\n",
       "\n",
       "[26303 rows x 11 columns]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The inputs of our model are meteorological data reported by KNMI (https://www.knmi.nl/home)\n",
    "knmi = pd.read_excel('/mnt/e/Education/M.Sc at UT/Projects/Energy consumption prediction/Iterative model/forGitHub/Datasets/WeatherData2016_2018.xlsx')\n",
    "knmi = knmi.set_index('Time') # for further processing, we need to set index of each smaple with Time\n",
    "knmi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_22091/3497004736.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  knmi_Updated['U'] = 0.01*knmi_Updated['U'] #Transfering percentage to fraction\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>month</th>\n",
       "      <th>HH</th>\n",
       "      <th>U</th>\n",
       "      <th>Temp</th>\n",
       "      <th>Q</th>\n",
       "      <th>FF</th>\n",
       "      <th>P</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-01-01 01:00:00</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.82</td>\n",
       "      <td>6.6</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>10224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-01 02:00:00</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.83</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>10228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-01 03:00:00</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.91</td>\n",
       "      <td>5.9</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>10232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-01 04:00:00</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.96</td>\n",
       "      <td>4.2</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>10237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-01 05:00:00</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.98</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>10240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-31 19:00:00</th>\n",
       "      <td>12</td>\n",
       "      <td>19</td>\n",
       "      <td>0.93</td>\n",
       "      <td>8.7</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>10341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-31 20:00:00</th>\n",
       "      <td>12</td>\n",
       "      <td>20</td>\n",
       "      <td>0.92</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>10338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-31 21:00:00</th>\n",
       "      <td>12</td>\n",
       "      <td>21</td>\n",
       "      <td>0.89</td>\n",
       "      <td>8.2</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>10336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-31 22:00:00</th>\n",
       "      <td>12</td>\n",
       "      <td>22</td>\n",
       "      <td>0.94</td>\n",
       "      <td>7.6</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>10332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-31 23:00:00</th>\n",
       "      <td>12</td>\n",
       "      <td>23</td>\n",
       "      <td>0.94</td>\n",
       "      <td>7.6</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>10333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26303 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     month  HH     U  Temp  Q  FF      P\n",
       "Time                                                    \n",
       "2016-01-01 01:00:00      1   1  0.82   6.6  0  30  10224\n",
       "2016-01-01 02:00:00      1   2  0.83   7.0  0  40  10228\n",
       "2016-01-01 03:00:00      1   3  0.91   5.9  0  30  10232\n",
       "2016-01-01 04:00:00      1   4  0.96   4.2  0  20  10237\n",
       "2016-01-01 05:00:00      1   5  0.98   4.0  0  20  10240\n",
       "...                    ...  ..   ...   ... ..  ..    ...\n",
       "2018-12-31 19:00:00     12  19  0.93   8.7  0  30  10341\n",
       "2018-12-31 20:00:00     12  20  0.92   8.5  0  30  10338\n",
       "2018-12-31 21:00:00     12  21  0.89   8.2  0  40  10336\n",
       "2018-12-31 22:00:00     12  22  0.94   7.6  0  40  10332\n",
       "2018-12-31 23:00:00     12  23  0.94   7.6  0  40  10333\n",
       "\n",
       "[26303 rows x 7 columns]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knmi_Updated= knmi.loc[:, ~knmi.columns.isin([\"TD\",\"RH\",\"DR\",\"FX\"])] # ~ sign drops the columns we select\n",
    "knmi_Updated['U'] = 0.01*knmi_Updated['U'] #Transfering percentage to fraction\n",
    "knmi_Updated "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spliting data for training and testing train(80%) , test(20%)\n",
    "from sklearn.model_selection import train_test_split\n",
    "#Inputs: Updated Meteorological data (when TD,DR,FX,RH eliminated)\n",
    "#Output: Building energy consumption \n",
    "X_train, X_test, y_train, y_test = train_test_split(knmi_Updated , Energy, test_size = 0.2, random_state = 0)\n",
    "y_train\n",
    "from sklearn.preprocessing import StandardScaler #standardizes the data to a range in which the mean is equal to 0 and the standard deviation is 1. It assumes the data is normally distributed.\n",
    "sc1= StandardScaler() \n",
    "X1 = sc1.fit_transform(knmi_Updated)\n",
    "X1 = sc1.fit_transform(knmi_Updated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohammad/miniconda3/envs/py310/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.3934\n",
      "Epoch 2/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1739\n",
      "Epoch 3/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.1568\n",
      "Epoch 4/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1527\n",
      "Epoch 5/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 947us/step - loss: 0.1492\n",
      "Epoch 6/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 959us/step - loss: 0.1468\n",
      "Epoch 7/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 966us/step - loss: 0.1455\n",
      "Epoch 8/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1422\n",
      "Epoch 9/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1399\n",
      "Epoch 10/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 934us/step - loss: 0.1404\n",
      "Epoch 11/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 954us/step - loss: 0.1423\n",
      "Epoch 12/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1392 \n",
      "Epoch 13/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1390\n",
      "Epoch 14/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1392\n",
      "Epoch 15/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1382\n",
      "Epoch 16/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1358\n",
      "Epoch 17/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 972us/step - loss: 0.1381\n",
      "Epoch 18/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 936us/step - loss: 0.1347\n",
      "Epoch 19/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 934us/step - loss: 0.1341\n",
      "Epoch 20/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 952us/step - loss: 0.1370\n",
      "Epoch 21/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 932us/step - loss: 0.1360\n",
      "Epoch 22/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 991us/step - loss: 0.1371\n",
      "Epoch 23/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1367\n",
      "Epoch 24/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1351\n",
      "Epoch 25/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.1341\n",
      "Epoch 26/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1341\n",
      "Epoch 27/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1332\n",
      "Epoch 28/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 967us/step - loss: 0.1346\n",
      "Epoch 29/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 965us/step - loss: 0.1339\n",
      "Epoch 30/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 993us/step - loss: 0.1314\n",
      "Epoch 31/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1350\n",
      "Epoch 32/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.1327\n",
      "Epoch 33/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1336\n",
      "Epoch 34/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1341\n",
      "Epoch 35/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 939us/step - loss: 0.1322\n",
      "Epoch 36/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1319\n",
      "Epoch 37/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1308\n",
      "Epoch 38/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1334\n",
      "Epoch 39/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.1335\n",
      "Epoch 40/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.1298\n",
      "Epoch 41/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.1304\n",
      "Epoch 42/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.1326\n",
      "Epoch 43/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1315\n",
      "Epoch 44/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.1302\n",
      "Epoch 45/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1324\n",
      "Epoch 46/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1299\n",
      "Epoch 47/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1317\n",
      "Epoch 48/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1290\n",
      "Epoch 49/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1308\n",
      "Epoch 50/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.1304\n",
      "Epoch 51/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1301\n",
      "Epoch 52/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1276\n",
      "Epoch 53/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1300\n",
      "Epoch 54/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1287\n",
      "Epoch 55/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.1312\n",
      "Epoch 56/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1309\n",
      "Epoch 57/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1310\n",
      "Epoch 58/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1287\n",
      "Epoch 59/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.1274\n",
      "Epoch 60/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1281\n",
      "Epoch 61/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1294\n",
      "Epoch 62/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1284\n",
      "Epoch 63/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1315\n",
      "Epoch 64/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1313\n",
      "Epoch 65/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 993us/step - loss: 0.1293\n",
      "Epoch 66/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 940us/step - loss: 0.1286\n",
      "Epoch 67/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 975us/step - loss: 0.1298\n",
      "Epoch 68/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 993us/step - loss: 0.1307\n",
      "Epoch 69/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1303\n",
      "Epoch 70/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1279\n",
      "Epoch 71/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1306\n",
      "Epoch 72/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1296\n",
      "Epoch 73/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1279\n",
      "Epoch 74/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 988us/step - loss: 0.1278\n",
      "Epoch 75/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.1296\n",
      "Epoch 76/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.1280\n",
      "Epoch 77/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.1282\n",
      "Epoch 78/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.1255\n",
      "Epoch 79/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1286\n",
      "Epoch 80/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 971us/step - loss: 0.1274\n",
      "Epoch 81/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 943us/step - loss: 0.1293\n",
      "Epoch 82/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 949us/step - loss: 0.1285\n",
      "Epoch 83/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 948us/step - loss: 0.1310\n",
      "Epoch 84/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 945us/step - loss: 0.1287\n",
      "Epoch 85/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1279\n",
      "Epoch 86/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 976us/step - loss: 0.1291\n",
      "Epoch 87/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 917us/step - loss: 0.1266\n",
      "Epoch 88/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 948us/step - loss: 0.1268\n",
      "Epoch 89/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 935us/step - loss: 0.1262\n",
      "Epoch 90/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 952us/step - loss: 0.1288\n",
      "Epoch 91/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1280\n",
      "Epoch 92/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1271\n",
      "Epoch 93/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1270\n",
      "Epoch 94/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.1266\n",
      "Epoch 95/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1292\n",
      "Epoch 96/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1257\n",
      "Epoch 97/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 934us/step - loss: 0.1243\n",
      "Epoch 98/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 904us/step - loss: 0.1250\n",
      "Epoch 99/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 912us/step - loss: 0.1300\n",
      "Epoch 100/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 925us/step - loss: 0.1282\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "R2 :  0.8463149230643972\n",
      "MSE :  8.16026044067673\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import SimpleRNN, Dense\n",
    "\n",
    "\n",
    "# Splitting  dataset into training (80%) and test data (20%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X1, Energy, test_size=0.2, random_state=0, shuffle=False) #X1: Knmi_updated scaled\n",
    "y_train = y_train.values.ravel()\n",
    "y_test = y_test.values.ravel()\n",
    "\n",
    "# Scaling the data for RNN\n",
    "scaler_X = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "X_test_scaled = scaler_X.transform(X_test)\n",
    "\n",
    "y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1, 1))\n",
    "y_test_scaled = scaler_y.transform(y_test.reshape(-1, 1))\n",
    "\n",
    "# Reshape data for RNN (samples, time steps, features)\n",
    "X_train_scaled = X_train_scaled.reshape((X_train_scaled.shape[0], 1, X_train_scaled.shape[1]))\n",
    "X_test_scaled = X_test_scaled.reshape((X_test_scaled.shape[0], 1, X_test_scaled.shape[1]))\n",
    "\n",
    "# Building the RNN model\n",
    "Ghaziasgar_and_Pourfayaz_RNN = Sequential()\n",
    "Ghaziasgar_and_Pourfayaz_RNN.add(SimpleRNN(units=50, activation='relu', input_shape=(X_train_scaled.shape[1], X_train_scaled.shape[2])))\n",
    "Ghaziasgar_and_Pourfayaz_RNN.add(Dense(1))\n",
    "\n",
    "Ghaziasgar_and_Pourfayaz_RNN.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Fitting the model to the training data\n",
    "Ghaziasgar_and_Pourfayaz_RNN.fit(X_train_scaled, y_train_scaled, epochs=100, batch_size=32)\n",
    "\n",
    "# Predicting on the test data\n",
    "Predicted_Test_RNN_Scaled = Ghaziasgar_and_Pourfayaz_RNN.predict(X_test_scaled)\n",
    "Predicted_Test_RNN = scaler_y.inverse_transform(Predicted_Test_RNN_Scaled)\n",
    "\n",
    "# Testing the model's accuracy on the test data\n",
    "print('R2 : ',r2_score(y_test, Predicted_Test_RNN))\n",
    "print('MSE : ',mean_squared_error(y_test, Predicted_Test_RNN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohammad/miniconda3/envs/py310/lib/python3.10/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 882us/step - loss: 0.4173\n",
      "Epoch 2/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 884us/step - loss: 0.1697\n",
      "Epoch 3/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 868us/step - loss: 0.1536\n",
      "Epoch 4/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 952us/step - loss: 0.1518\n",
      "Epoch 5/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 789us/step - loss: 0.1490\n",
      "Epoch 6/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 793us/step - loss: 0.1505\n",
      "Epoch 7/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 867us/step - loss: 0.1453\n",
      "Epoch 8/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 843us/step - loss: 0.1454\n",
      "Epoch 9/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 896us/step - loss: 0.1396\n",
      "Epoch 10/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 851us/step - loss: 0.1458\n",
      "Epoch 11/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 877us/step - loss: 0.1426\n",
      "Epoch 12/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 890us/step - loss: 0.1415\n",
      "Epoch 13/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 883us/step - loss: 0.1407\n",
      "Epoch 14/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 885us/step - loss: 0.1377\n",
      "Epoch 15/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 894us/step - loss: 0.1416\n",
      "Epoch 16/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 880us/step - loss: 0.1399\n",
      "Epoch 17/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 888us/step - loss: 0.1362\n",
      "Epoch 18/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 880us/step - loss: 0.1421\n",
      "Epoch 19/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 904us/step - loss: 0.1380\n",
      "Epoch 20/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 948us/step - loss: 0.1342\n",
      "Epoch 21/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 879us/step - loss: 0.1364\n",
      "Epoch 22/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 901us/step - loss: 0.1345\n",
      "Epoch 23/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 840us/step - loss: 0.1375\n",
      "Epoch 24/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 862us/step - loss: 0.1353\n",
      "Epoch 25/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 857us/step - loss: 0.1337\n",
      "Epoch 26/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 850us/step - loss: 0.1339\n",
      "Epoch 27/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 876us/step - loss: 0.1325\n",
      "Epoch 28/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 891us/step - loss: 0.1320\n",
      "Epoch 29/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1338\n",
      "Epoch 30/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 926us/step - loss: 0.1319\n",
      "Epoch 31/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 713us/step - loss: 0.1309\n",
      "Epoch 32/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 932us/step - loss: 0.1309\n",
      "Epoch 33/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 978us/step - loss: 0.1326\n",
      "Epoch 34/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 985us/step - loss: 0.1321\n",
      "Epoch 35/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 973us/step - loss: 0.1300\n",
      "Epoch 36/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 858us/step - loss: 0.1310\n",
      "Epoch 37/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 946us/step - loss: 0.1343\n",
      "Epoch 38/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 968us/step - loss: 0.1302\n",
      "Epoch 39/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 919us/step - loss: 0.1310\n",
      "Epoch 40/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 791us/step - loss: 0.1295\n",
      "Epoch 41/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 756us/step - loss: 0.1298\n",
      "Epoch 42/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 801us/step - loss: 0.1328\n",
      "Epoch 43/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 851us/step - loss: 0.1331\n",
      "Epoch 44/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 837us/step - loss: 0.1309\n",
      "Epoch 45/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 843us/step - loss: 0.1302\n",
      "Epoch 46/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 833us/step - loss: 0.1282\n",
      "Epoch 47/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 877us/step - loss: 0.1289\n",
      "Epoch 48/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 895us/step - loss: 0.1292\n",
      "Epoch 49/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 876us/step - loss: 0.1259\n",
      "Epoch 50/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1296\n",
      "Epoch 51/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 794us/step - loss: 0.1268\n",
      "Epoch 52/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 745us/step - loss: 0.1293\n",
      "Epoch 53/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 791us/step - loss: 0.1271\n",
      "Epoch 54/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 788us/step - loss: 0.1276\n",
      "Epoch 55/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 835us/step - loss: 0.1288\n",
      "Epoch 56/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 662us/step - loss: 0.1281\n",
      "Epoch 57/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 734us/step - loss: 0.1275\n",
      "Epoch 58/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 803us/step - loss: 0.1294\n",
      "Epoch 59/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 789us/step - loss: 0.1299\n",
      "Epoch 60/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 791us/step - loss: 0.1278\n",
      "Epoch 61/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 801us/step - loss: 0.1310\n",
      "Epoch 62/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 805us/step - loss: 0.1312\n",
      "Epoch 63/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 789us/step - loss: 0.1280\n",
      "Epoch 64/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 853us/step - loss: 0.1284\n",
      "Epoch 65/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 788us/step - loss: 0.1264\n",
      "Epoch 66/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 790us/step - loss: 0.1268\n",
      "Epoch 67/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 813us/step - loss: 0.1271\n",
      "Epoch 68/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 810us/step - loss: 0.1267\n",
      "Epoch 69/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 796us/step - loss: 0.1291\n",
      "Epoch 70/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 791us/step - loss: 0.1283\n",
      "Epoch 71/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 806us/step - loss: 0.1275\n",
      "Epoch 72/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 789us/step - loss: 0.1285\n",
      "Epoch 73/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 851us/step - loss: 0.1281\n",
      "Epoch 74/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1254\n",
      "Epoch 75/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1275\n",
      "Epoch 76/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 987us/step - loss: 0.1289\n",
      "Epoch 77/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1271\n",
      "Epoch 78/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 967us/step - loss: 0.1274\n",
      "Epoch 79/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 867us/step - loss: 0.1294\n",
      "Epoch 80/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 929us/step - loss: 0.1274\n",
      "Epoch 81/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1263\n",
      "Epoch 82/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.1266\n",
      "Epoch 83/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.1278\n",
      "Epoch 84/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 835us/step - loss: 0.1283\n",
      "Epoch 85/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 842us/step - loss: 0.1269\n",
      "Epoch 86/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 945us/step - loss: 0.1247\n",
      "Epoch 87/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 844us/step - loss: 0.1247\n",
      "Epoch 88/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 834us/step - loss: 0.1261\n",
      "Epoch 89/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 837us/step - loss: 0.1257\n",
      "Epoch 90/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 883us/step - loss: 0.1246\n",
      "Epoch 91/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 870us/step - loss: 0.1259\n",
      "Epoch 92/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 891us/step - loss: 0.1239\n",
      "Epoch 93/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 853us/step - loss: 0.1235\n",
      "Epoch 94/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 835us/step - loss: 0.1261\n",
      "Epoch 95/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 885us/step - loss: 0.1245\n",
      "Epoch 96/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 894us/step - loss: 0.1260\n",
      "Epoch 97/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 862us/step - loss: 0.1247\n",
      "Epoch 98/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 870us/step - loss: 0.1246\n",
      "Epoch 99/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 887us/step - loss: 0.1257\n",
      "Epoch 100/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 897us/step - loss: 0.1259\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 764us/step\n",
      "R2 :  0.8462949064836035\n",
      "MSE :  8.16132326678623\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Splitting dataset into training (80%) and test data (20%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X1, Energy , test_size=0.2, random_state=0, shuffle=False) #X1: Knmi_updated scaled\n",
    "y_train = y_train.values.ravel()\n",
    "y_test = y_test.values.ravel()\n",
    "\n",
    "# Scaling the data for ANN\n",
    "scaler_X = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "X_test_scaled = scaler_X.transform(X_test)\n",
    "\n",
    "y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1, 1))\n",
    "y_test_scaled = scaler_y.transform(y_test.reshape(-1, 1))\n",
    "\n",
    "# Building the ANN model\n",
    "Ghaziasgar_and_Pourfayaz_ANN = Sequential()\n",
    "Ghaziasgar_and_Pourfayaz_ANN.add(Dense(units=50, activation='relu', input_dim=X_train_scaled.shape[1]))\n",
    "Ghaziasgar_and_Pourfayaz_ANN.add(Dense(units=1))  # Output layer\n",
    "\n",
    "Ghaziasgar_and_Pourfayaz_ANN.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Fitting the model to the training data\n",
    "Ghaziasgar_and_Pourfayaz_ANN.fit(X_train_scaled, y_train_scaled, epochs=100, batch_size=32)\n",
    "\n",
    "# Predicting on the test data\n",
    "Predicted_Test_ANN_Scaled = Ghaziasgar_and_Pourfayaz_ANN.predict(X_test_scaled)\n",
    "Predicted_Test_ANN = scaler_y.inverse_transform(Predicted_Test_ANN_Scaled)\n",
    "\n",
    "# Testing the model's accuracy on the test data\n",
    "print('R2 : ',r2_score(y_test, Predicted_Test_ANN))\n",
    "print('MSE : ',mean_squared_error(y_test, Predicted_Test_ANN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohammad/miniconda3/envs/py310/lib/python3.10/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.2728\n",
      "Epoch 2/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1492\n",
      "Epoch 3/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1404\n",
      "Epoch 4/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1368\n",
      "Epoch 5/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1307\n",
      "Epoch 6/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.1278\n",
      "Epoch 7/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.1280\n",
      "Epoch 8/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1234\n",
      "Epoch 9/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.1180\n",
      "Epoch 10/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1161\n",
      "Epoch 11/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1178\n",
      "Epoch 12/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1142\n",
      "Epoch 13/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1128\n",
      "Epoch 14/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1074\n",
      "Epoch 15/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1071\n",
      "Epoch 16/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1040\n",
      "Epoch 17/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1056\n",
      "Epoch 18/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1039\n",
      "Epoch 19/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0992\n",
      "Epoch 20/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1039\n",
      "Epoch 21/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0992\n",
      "Epoch 22/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0975\n",
      "Epoch 23/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0957\n",
      "Epoch 24/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0944\n",
      "Epoch 25/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0920\n",
      "Epoch 26/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0950\n",
      "Epoch 27/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0926\n",
      "Epoch 28/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0939\n",
      "Epoch 29/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0941\n",
      "Epoch 30/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1000us/step - loss: 0.0902\n",
      "Epoch 31/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0912\n",
      "Epoch 32/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0878\n",
      "Epoch 33/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0876\n",
      "Epoch 34/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0879\n",
      "Epoch 35/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0851\n",
      "Epoch 36/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0864\n",
      "Epoch 37/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0846\n",
      "Epoch 38/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0830\n",
      "Epoch 39/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0830\n",
      "Epoch 40/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0841\n",
      "Epoch 41/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0806\n",
      "Epoch 42/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0794\n",
      "Epoch 43/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0805\n",
      "Epoch 44/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0818\n",
      "Epoch 45/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0800\n",
      "Epoch 46/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0782\n",
      "Epoch 47/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0777\n",
      "Epoch 48/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0771\n",
      "Epoch 49/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0776\n",
      "Epoch 50/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0776\n",
      "Epoch 51/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0786\n",
      "Epoch 52/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0761\n",
      "Epoch 53/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0742\n",
      "Epoch 54/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0741\n",
      "Epoch 55/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0734\n",
      "Epoch 56/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0725\n",
      "Epoch 57/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0736\n",
      "Epoch 58/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0735\n",
      "Epoch 59/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0742\n",
      "Epoch 60/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0702\n",
      "Epoch 61/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0723\n",
      "Epoch 62/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0733\n",
      "Epoch 63/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0697\n",
      "Epoch 64/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0692 \n",
      "Epoch 65/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0693\n",
      "Epoch 66/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0683\n",
      "Epoch 67/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0681\n",
      "Epoch 68/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0675\n",
      "Epoch 69/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0680\n",
      "Epoch 70/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0677\n",
      "Epoch 71/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0666\n",
      "Epoch 72/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0672\n",
      "Epoch 73/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0661\n",
      "Epoch 74/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0659\n",
      "Epoch 75/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0651\n",
      "Epoch 76/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0676\n",
      "Epoch 77/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0676\n",
      "Epoch 78/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0655\n",
      "Epoch 79/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0636\n",
      "Epoch 80/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0647\n",
      "Epoch 81/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0648\n",
      "Epoch 82/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0643\n",
      "Epoch 83/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0638\n",
      "Epoch 84/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0624\n",
      "Epoch 85/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0630\n",
      "Epoch 86/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0607\n",
      "Epoch 87/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0609\n",
      "Epoch 88/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0628\n",
      "Epoch 89/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0606\n",
      "Epoch 90/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0616\n",
      "Epoch 91/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0608\n",
      "Epoch 92/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0610\n",
      "Epoch 93/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0615\n",
      "Epoch 94/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0597\n",
      "Epoch 95/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0609\n",
      "Epoch 96/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0598\n",
      "Epoch 97/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0593\n",
      "Epoch 98/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0603\n",
      "Epoch 99/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0588\n",
      "Epoch 100/100\n",
      "\u001b[1m658/658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0596\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step  \n",
      "R2 :  0.8071163938329533\n",
      "MSE :  10.241595947012804\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "\n",
    "# Splitting dataset into training (80%) and test data (20%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X1, Energy, test_size=0.2, random_state=0, shuffle=False) #X1: Knmi_updated scaled\n",
    "y_train = y_train.values.ravel()\n",
    "y_test = y_test.values.ravel()\n",
    "\n",
    "# Scaling the data for DNN\n",
    "scaler_X = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "X_test_scaled = scaler_X.transform(X_test)\n",
    "\n",
    "y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1, 1))\n",
    "y_test_scaled = scaler_y.transform(y_test.reshape(-1, 1))\n",
    "\n",
    "# Building the DNN model\n",
    "Ghaziasgar_and_Pourfayaz_DNN = Sequential()\n",
    "\n",
    "# Adding layers to the DNN\n",
    "Ghaziasgar_and_Pourfayaz_DNN.add(Dense(units=128, activation='relu', input_dim=X_train_scaled.shape[1]))\n",
    "Ghaziasgar_and_Pourfayaz_DNN.add(Dense(units=64, activation='relu'))\n",
    "Ghaziasgar_and_Pourfayaz_DNN.add(Dense(units=32, activation='relu'))\n",
    "Ghaziasgar_and_Pourfayaz_DNN.add(Dense(units=16, activation='relu'))\n",
    "Ghaziasgar_and_Pourfayaz_DNN.add(Dense(units=1))  # Output layer for regression\n",
    "\n",
    "Ghaziasgar_and_Pourfayaz_DNN.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Fitting the model to the training data\n",
    "Ghaziasgar_and_Pourfayaz_DNN.fit(X_train_scaled, y_train_scaled, epochs=100, batch_size=32)\n",
    "\n",
    "# Predicting on the test data\n",
    "Predicted_Test_DNN_Scaled = Ghaziasgar_and_Pourfayaz_DNN.predict(X_test_scaled)\n",
    "Predicted_Test_DNN = scaler_y.inverse_transform(Predicted_Test_DNN_Scaled)\n",
    "\n",
    "# Testing the model's accuracy on the test data\n",
    "print('R2 : ', r2_score(y_test, Predicted_Test_DNN))\n",
    "print('MSE : ', mean_squared_error(y_test, Predicted_Test_DNN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>R2_Score</th>\n",
       "      <th>MSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>RNN</th>\n",
       "      <td>0.846315</td>\n",
       "      <td>8.160260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ANN</th>\n",
       "      <td>0.846295</td>\n",
       "      <td>8.161323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DNN</th>\n",
       "      <td>0.807116</td>\n",
       "      <td>10.241596</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     R2_Score        MSE\n",
       "RNN  0.846315   8.160260\n",
       "ANN  0.846295   8.161323\n",
       "DNN  0.807116  10.241596"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R2Score_Models = {\n",
    "    'RNN': r2_score(y_test, Predicted_Test_RNN),\n",
    "    'ANN' : r2_score(y_test, Predicted_Test_ANN),\n",
    "    'DNN': r2_score(y_test, Predicted_Test_DNN)\n",
    "\n",
    "}\n",
    "\n",
    "MSE_Models = {\n",
    "    'RNN': mean_squared_error(y_test, Predicted_Test_RNN),\n",
    "    'ANN' : mean_squared_error(y_test, Predicted_Test_ANN),\n",
    "    'DNN': mean_squared_error(y_test, Predicted_Test_DNN)\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "# Combine R2 and MSE into a single DataFrame\n",
    "metrics_df = pd.DataFrame({\n",
    "    'R2_Score': R2Score_Models,\n",
    "    'MSE': MSE_Models\n",
    "})\n",
    "metrics_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
